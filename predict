#!/bin/bash -l
#SBATCH --partition=gpudev
#SBATCH --gres=gpu:a100:1
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=18
#SBATCH --mem=125G
#SBATCH --time=00:15:00
#SBATCH -o jobfiles/out.%j
#SBATCH -e jobfiles/err.%j
#SBATCH -D /raven/u/sarlt/lora           # project directory
#SBATCH -J predict

export PYTHONNOUSERSITE=1
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export NCCL_ASYNC_ERROR_HANDLING=1

module purge
module load anaconda/3/2023.03           # Python‑3.10
module load gcc/12
module load cuda/12.1
module load openmpi_gpu/4.1

eval "$(conda shell.bash hook)"
conda activate lora

module load pytorch-distributed/gpu-cuda-12.1/2.2.0
export PATH="$CONDA_PREFIX/bin:$PATH"
export PYTHONPATH="$CONDA_PREFIX/lib/python3.10/site-packages:${PYTHONPATH:-}"
# env’s C/C++ runtime libs first
export LD_LIBRARY_PATH="$CONDA_PREFIX/lib:${LD_LIBRARY_PATH:-}"

export HF_HUB_OFFLINE=1


# ---- quick sanity printout ---------------------------------------
echo "Python interpreter : $(which python)"
python - <<'PY'
import sys, torch, transformers, importlib.util
print("PyTorch      :", torch.__version__, torch.__file__)
print("Transformers :", transformers.__version__)
print("Python       :", sys.version.replace("\n"," "))
import peft
print("PEFT         :", peft.__version__, peft.__file__)

PY
echo "---------------------------------------------------------------"

# ---- launch ------------------------------------------------------
python -m torch.distributed.run \
       --standalone \
       --nnodes 1 \
       --nproc_per_node 1 \
       predict.py
